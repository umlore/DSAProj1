Project Report
Eugene Umlor and Kathryn Lovell

Our implementation of the distributed twitter problem is mostly complete, but is frustrated by a few message passing exceptions.  We were unable to get messages to send through the socket channels we established between machines, which was the largest obstacle to our progress, and this report will go into greater detail later.  However, we were able to successfully establish a great deal of functionality.
Each machine in the twitter network is assigned an instance of WuuInstance.  This performs the functions of message passing and receiving, log maintenance, blocklist maintenance, log truncation, and establishing connections back and forth from other instances.
Establishing connections to other instances is performed in a separate thread from the main listening execution.  This means that new instances can, at any time, join the network and begin participating in message exchange.  Regular function of the WuuInstance is not blocked or otherwise paused until all machines have connected.  If an instance crashes and reconnects, the closed sockets will be updated.  The instance that has restarted will have a timestamp matrix that is reset to 0 in all indices, but other instances will still know that the restarted instance knows about all messages until the point at which it crashed.  So, a restarted instance will only be able to view some of the tweets that occured concurrently to its last, and all of the tweets that happened logically after its last.  This is fixable by setting all the values in the timestamp matrix that correspond to a connected instance when it accepts a connection, in case the connection has been reset and the instance needs to re-receive all the old messages.
Log truncation needs to be adapted so that in the event of a crash, it will not remove all block messsages that all other computers in the network know about - only the block messages with a corresponding unblock message that all other computers in the network know about.  This way, in the event of a crash, restarted instances will not miss block instructions that all instances previously knew about.  We did not write the stored log to a file, but storing the log on disk would also solve crash related issues.
Command-line input on a computer to issue commands is currently not possible, as that code was eaten by a well-intentioned version control error.  However, we had created a multithreaded implementation of reading command-line instructions.  This meant that in total, there were exactly three threads operating at any given time - one main thread which constantly listened for messages from other machines, as well as checking for issued instructions from the two other threads, one secondary thread which constantly listened for other instances to connect and sent connection requests back when it received them, and one final thread to parse user input and perform actions.
We have developed prototypes for the log maintenance and blocklist dictionary maintenance functionality, which should work in the event that messages get through.
Our biggest problem was simply that while we could write to the client machine's endpoint, we could not get the read buffer in the socket to stay open for subsequent messages. It seems to be baked into the design of Java
that a BufferedInputStream cannot be used more than once; the socket closes it after a single read, and closing the reader inherently closes the socket. Thus, Java sockets must be re-created for every message, which we wanted to avoid. Unfortunately, we didn't figure out we couldn't avoid it in time. We had suspicions that our ThreadExecutor, which controls the deployment of our three concurrent threads, was interacting incorrectly in a linux environment, but there was nothing we can do about this. Simply running Thread.start() causes the same issue.
That our instances are both "clients" and "servers" posed some unique challenges, particularly aound a socket infrastructure that is inset in this dichotomy. We needed to run separate threads to accomodate both sending and processing input and receiving it, and this is another layer on top of the distribution of the EC2 instances. For example, when attempting to connect to a "client" right after the client accepted a connection, we must tell the instance to wait so that the client can be prepared to once again receive connections. There is only one ServerSocket, so we can only accept one connection at a time.
Multithreading our server was also a significant obstacle - initially we tried to avoid blocking issues by having our thread implement java's Callable interface, and having it return a Future.  The issue was that we could not check and see if this value had been updated more than once, as future.get could only be called one time.  We later experimented switching between callables and runnables, and eventually determined that the best course of action was to simply have the thread update boolean flags directly.